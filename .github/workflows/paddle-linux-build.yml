name: paddle-linux-build
on:
  push:
    branches:
      - main
    paths:
      - '.github/workflows/paddle-linux-build.yml'
  workflow_dispatch:

env:
  PADDLE_VERSION: "2.6.1"
  PADDLE_BRANCH: "release/2.6"

jobs:
  build-all:
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        include:
          - { name: openblas_noavx, cmake_flags: "-DWITH_AVX=OFF -DWITH_MKL=OFF" }
          - { name: openblas,       cmake_flags: "-DWITH_AVX=ON  -DWITH_MKL=OFF" }
          - { name: mkldnn,         cmake_flags: "-DWITH_AVX=ON  -DWITH_MKL=ON"  }
    steps:
      - name: Setup dependencies
        run: |
          pip3 install numpy protobuf wheel ninja

      - name: Checkout Paddle repo
        uses: actions/checkout@v4
        with:
          repository: PaddlePaddle/Paddle
          ref: ${{ env.PADDLE_BRANCH }}
          path: paddle-src
          fetch-depth: 1

      - name: Configure with CMake
        run: |
          mkdir -p ~/paddle-src/build
          cd ~/paddle-src/build
          cmake .. -GNinja \
            -DWITH_AVX=${{ matrix.AVX_FLAG }} \
            -DWITH_MKL=${{ matrix.MKL_FLAG }} \
            -DWITH_GPU=OFF \
            -DON_INFER=ON \
            -DWITH_PYTHON=OFF \
            -DWITH_ONNXRUNTIME=ON \
            -DWITH_UNITY_BUILD=ON

      - name: Build with Ninja
        run: |
          cd ~/paddle-src/build
          ninja all

      - name: Compress paddle_inference_c_install_dir
        run: |
          cd ~/paddle-src/build
          tar -czf paddle_inference_c_${{ matrix.name }}.tar.gz paddle_inference_c_install_dir/

      - name: Compress paddle_inference_install_dir
        run: |
          cd ~/paddle-src/build
          tar -czf paddle_inference_cpp_${{ matrix.name }}.tar.gz paddle_inference_install_dir/

      - name: Upload paddle_inference_c_install_dir
        uses: actions/upload-artifact@v4
        with:
          name: c_${{ matrix.name }}
          path: ~/paddle-src/build/paddle_inference_c_${{ matrix.name }}.tar.gz

      - name: Upload paddle_inference_install_dir
        uses: actions/upload-artifact@v4
        with:
          name: cpp_${{ matrix.name }}
          path: ~/paddle-src/build/paddle_inference_cpp_${{ matrix.name }}.tar.gz
